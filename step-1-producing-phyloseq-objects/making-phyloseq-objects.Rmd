---
title: "making-phyloseq-objects-using-dada2"
author: "Judy-Malas"
date: "12/17/2021"
output: 
  html_document: 
    theme: readable
---

Tips before starting:

* Make sure you have the lastest version of R, this Markdown was made on R version 4.1.1 
* Some of these steps take a while, make sure you change your computer settings so that it doesn't automatically sleep during one of the steps
 + That said, if your comp doesn't have a whole lot of memory, don't multitask while running the code chunks so your comp doesn't crash
* Save individual files made between steps using the `saveRDS` function to your data folder
* Sometimes if you're getting werid error messages for a while, it's best to restart R, or better yet, restart R and go do something else for a while
* Most packages have websites or repositories on github, if you're really stuck, that would be a good place to look. some examples:
 + https://github.com/joey711/phyloseq/issues
 + https://github.com/benjjneb/dada2/issues
 + http://www2.decipher.codes/Classification.html
 
R Markdown tips 

* knitting the document produces a nice readable html
* all code chunks should have unique names and should be set to `eval = FALSE` unless you want that code chunk to run when you knit the document  
  + you probably only want code chunks that take low computational power to run and if you want the plots or output from those chunks to show up 
  + if that chunk depends on objects from an earlier chunk, you must save the output of that earlier chunk into your data folder, and then re-load it in the desired code chunk



Uncomment and run this chunk only if you don't have the packages needed in the next code chunk 
```{r install_packages, eval = FALSE }

#if (!requireNamespace("BiocManager", quietly = TRUE))
 #install.packages("BiocManager")
#BiocManager::install(version = "3.14")


#if (!requireNamespace("BiocManager", quietly = TRUE))
 #   install.packages("BiocManager")

#BiocManager::install("dada2", force = TRUE)

#if (!requireNamespace("BiocManager", quietly=TRUE))
 #   install.packages("BiocManager")
# BiocManager::install("DECIPHER")


#BiocManager::install("DESeq2")

#BiocManager::install("microbiome")


#install.packages("phangorn")

```

It doesn't matter if the package names have quotes or not
```{r load-packages, message=FALSE, warning=FALSE}


library(dada2); packageVersion("dada2")
library(ggplot2)
library(gridExtra)
library(phyloseq)
library(DECIPHER)
library(phangorn)
require("tidyverse")
library("dplyr")
library("rmarkdown")
library("vegan")
library("PoiClaClu")
library("doParallel")
library("plotly")
library("microbiome")
library("DESeq2")
library("ggpubr")
library("viridis")
library("plyr")
library("magrittr")
library("scales")
#library("data.table")
#library(grid)
#library(reshape2)
#library(randomForest)

```


## STEPS 
* Step 1: Getting the files ready
* Step 2: Quality checking 
  + Step 2.1: Filtering
  + Step 2.2: Checking quality after filtering 
* Step 3: Dereplication step
* Step 4: Error rates
* STEP 5: Construct ASVS with `DADA2` 
* Step 6: Make a sequence table and remove chimeras
* Step 7: Assign taxonomy, using the chimera-free reads
  + Step 7.1: Assign taxonomy another way with `DECIPHER` , fits with the variance stabilizing transformation in step 10 
* Step 8: Make a tree with `phangorn`
* Step 9:  Read in in your metadata table
* Step 10: Variance stabilizing transformation; ignore if not transforming data
* Step 11: Make a phyloseq object on transformed data
  + Step 11.1: Make a phyloseq object w/o transformed data
  
Important files made in part one:

  `all` = phyloseq object w/o variance transformation, live samples full time of experiment
  
  `vst_all` = phyloseq object WITH variance transformation, all samples full time of experiment 

> based mainly on https://benjjneb.github.io/dada2/tutorial.html 
  
----

### STEP 1: GETTING FILES READY
Live samples: Create a path to the files being used
```{r fastq-file-path}

all_live_path = file.path("~/landfill-microcosms/data/PEAR-merged-fastq-files/all_live")
all_filt_path <- file.path("~/Desktop/Landfill_Project/16s_data/all_live_filt_from_github_repository/")

```

```{r read-in-file-names}

fns_all <- sort(list.files(all_live_path, full.names = TRUE)) #sort ensures forward/reverse reads are in the same order

```


### STEP 2: QUALITY CHECKING

Live Samples: Check quality of paired reads 

Look at the quality plot of one sample first to make sure this step works. Checking all of the plots in the following step will take a long time to run
```{r qualitycheck-one}

dada2::plotQualityProfile("~/landfill-microcosms/data/PEAR-merged-fastq-files/all_live/AB1_t1_PEAR.assembled.fastq")

```


```{r  qualitycheck-merged-reads, eval=FALSE}

alli <- sample(length(fns_all), 83) #the number should be the number of files in the directory with fastq files

for(i in alli) { 
  print(plotQualityProfile(fns_all[i]))  
  } 

```

> In gray-scale is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length, hence the flat red line). (https://benjjneb.github.io/dada2/tutorial.html)

> What is a Quality Score?
A quality score (Q-score) is a prediction of the probability of an error in
base calling. It serves as a compact way to communicate very small
error probabilities.
A high quality score implies that a base call is more reliable and less
likely to be incorrect. For example, for base calls with a quality score
of Q40, one base call in 10,000 is predicted to be incorrect. For base
calls with a quality score of Q30, one base call in 1,000 is predicted
to be incorrect. Table 1 shows the relationship between the base call
quality scores and their corresponding error probabilities (https://www.illumina.com/content/dam/illumina-marketing/documents/products/technotes/technote_understanding_quality_scores.pdf)


####  STEP 2.1: FILTERING

We use standard filtering parameters, the most important being the enforcement of a maximum of 2 expected errors per-read [https://academic.oup.com/bioinformatics/article/31/21/3476/194979].

We also trimmed first and last 10 bp because these are likely to be errors [Callahan et al. 2016]

This step will take a while
```{r live-filter-and-trim, eval=FALSE}

#all_live 

if(!file_test("-d", all_live_path)) dir.create(all_filt_path) 

filt_all <- file.path(all_filt_path, basename(fns_all))


for(i in seq_along(fns_all)) {filterAndTrim(fns_all, filt_all, 
                rev = NULL, 
                filt.rev =NULL, 
                trimLeft=c(10), truncLen=c(290),
                maxN=0, maxEE=c(2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread=TRUE)}

```
When pushing changes to github after this step, be careful not to select the filtered files.. this will probably take a long time and isn't necessary.. it is a good idea to keep them saved on your local comp or hard-drive. 
When you go to the get hub plane and select "Diff", just don't check any of the files (or the entire datafile), from here on out just save the files you need for version control. 
If you ever lose the filtered files you can come back and repeat this filter and trim step AS LONG AS YOU HAVE THE ORIGINAL FASTQC FILES. DON'T LOSE THOSE. UPLOAD THEM TO A CLOUD OR MULTIPLE HARDRIVES.

#### STEP 2.2: QUALITY CHECK AFTER FILTERING

```{r check-one-trimmmed-quality}
print(plotQualityProfile("~/landfill-microcosms/data/PEAR-merged-fastq-files/all_live_filt/AB1_t2_PEAR.assembled.fastq"))

```


```{r check-trimmed-quality, eval=FALSE}

all_filt <- sample(length(filt_all), 83) 
for(i in all_filt) { print(plotQualityProfile(filt_all[i]) + ggtitle("All live filt"))}


```

### STEP 3: DEPEPLICATION 
Live Samples: Dereplication
```{r, eval=FALSE}

derep_all = derepFastq(filt_all)

sample.names <- basename(fns_all)

names(derep_all) <- sample.names
#sam.names_all <- sapply(strsplit(basename(filt_all), "_"), `[`, 1)

#names(derep_all) <- sam.names_all



```
> Here we use the high-resolution DADA2 method to infer ribosomal seq variats exactly, without imposing any arbitatry thershold (as is typical with OTUs) (Callahan et al., 2016)


### STEP 4: ERROR RATES
For learning errors, used code here because the one in the Callahan et al. paper didn't work, but this is by the same author:
```{r learn-errors, eval=FALSE}



error_all = learnErrors(derep_all, multithread = TRUE) # by default the learnErrors function uses only a subset of the data (the first 100 M bases); can increase if desired. 

#err_all = learnErrors(derep_all, multithread = TRUE, nbases = 1e9) 

saveRDS(error_all, "../data/live_samples_objects/error_all")

```

> The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).https://benjjneb.github.io/dada2/tutorial_1_8.html


```{r plot-errors}

error_all <- readRDS("../data/error_all")

plotErrors(error_all, nominalQ=TRUE) 

```


### STEP 5: CONSTRUCT ASVS WITH DADA2 

```{r dereplication-live, eval=FALSE}

dada_all = dada(derep_all, err=error_all, pool=TRUE, multithread= TRUE)




```

### STEP 6: SEQ TABLE + REMOVE CHIMERAS
Live samples:Construct Sequence table + Remove Chimeras 
```{r make_sequence_table, eval=FALSE}

seqtab = makeSequenceTable(dada_all)

nochim_seqtab.all <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

saveRDS(nochim_seqtab.all, "~/landfill-microcosms/data/live_samples_objects/nochim_seqtab.all")

```

### STEP 7: CLASSIFY TAXONOMY WITH DADA2
Live samples: Classify Taxonomy
The training datasets are too large to upload to github, so it's best to keep the files on your computer 
I used the `DECIPHER` package in the next step to assign the taxonomy, it is faster
```{r classify-taxonomy, eval=FALSE}

#ref_fasta = "~/Desktop/Landfill_Project/16s_data/silva_nr_v138_train_set.fa.gz"
#print(ref_fasta)

#taxtab.all <- assignTaxonomy(nochim_seqtab.all, refFasta = ref_fasta)
#colnames(taxtab.all) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus")

```


This is a continuation of the previous step, don't think it can be used with DEPICPHER step below
```{r add-species, eval = FALSE}

#taxa <- addSpecies(taxa, "~/tax/silva_species_assignment_v132.fa.gz") #change file path 

```


### STEP 7.1: ASSIGN TAXONOMY WITH DECIPHER. 
Code from here:http://www2.decipher.codes/ClassifyOrganismsCode.html

Download the training dataset for organismal classification: http://www2.decipher.codes/Downloads.html

I used SILVA SSU r138 (modified)

```{r classify-taxomy-decipher, eval = FALSE}

dna <- DNAStringSet(getSequences(nochim_seqtab.all)) # Create a DNAStringSet from the ASVs

load("~/Desktop/Landfill_Project/16s_data/SILVA_SSU_r138_2019.RData") # CHANGE TO THE PATH OF YOUR TRAINING SET

ids <- DECIPHER::IdTaxa(dna,
   trainingSet,
   strand="top",
   threshold = 60, # 60 is default
   processors = NULL) # use all available processors


```

```{r convert-ids-to-usable-format, eval = FALSE}


ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species") # ranks of interest

# Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy

taxid <- t(sapply(ids, function(x) {
        m <- match(ranks, x$rank)
        taxa <- x$taxon[m]
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))

colnames(taxid) <- ranks; rownames(taxid) <- getSequences(nochim_seqtab.all)

saveRDS(taxid, "../data/taxid") # save the converted taxa table to the data folder

```


### STEP 8: MAKE A TREE 
This step is optional, but why not give it a try
```{r Live-Construct-phylogenetic-tree, eval=FALSE}

# Tree for all samples
all_seq <- getSequences(nochim_seqtab.all)

names(all_seq) <- all_seq

alignment_all <- AlignSeqs(DNAStringSet(all_seq), anchor=NA) #multiple seq alignment

all_phang.align<- phyDat(as(alignment_all, "matrix"), type="DNA") #The phangorn R package is then used to construct a phylogenetic tree.
#neighborjoining tree

all_dm <- dist.ml(all_phang.align) 
all_treeNJ <- NJ(all_dm) # Note, tip order != sequence order
all_fit = pml(all_treeNJ, data=all_phang.align)
#GTR+G+I tree
all_fitGTR <- update(all_fit, k=4, inv=0.2)
all_fitGTR <- optim.pml(all_fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)

saveRDS(all_fitGTR, "../data/all_fitGTR") #save the tree to the data folder


```

### Step 9: Load metadata
+ make sure metadata files match the seq tab files
```{r, eval=FALSE}

all_metadata = read.csv("../data/metadata/allmetadata.csv")

#all 
all(rownames(nochim_seqtab.all) %in% all_metadata$sample)

rownames(all_metadata) = all_metadata$sample

rownames(nochim_seqtab.all) <- c("AB1","AB1.1","AB1.2","AB1.3","AB1.4","AB1.5", "AB2",   "AB2.1", "AB2.2", "AB2.3", "AB2.4" ,"AB2.5", "AB3"  , "AB3.1", "AB3.2", "AB3.3" ,"AB3.4","Fe1", "Fe1.1", "Fe1.2", "Fe1.3", "Fe1.4", "Fe1.5", "Fe2", "Fe2.1", "Fe2.2", "fe2", "Fe2.3", "Fe2.4", "Fe3", "Fe3.1", "Fe3.2", "Fe3.3", "Fe3.4", "Fe3.5", "NL1", "NL1.1", "NL1.2", "NL1.3", "NL2", "NL2.1", "NL2.2", "NL2.3", "NL3", "NL3.1", "NL3.2", "NL3.3", "NL4", "NL4.1", "NL4.2", "NL4.3", "NL4.4", "NL4.5", "NL5", "NL5.1", "NL5.2", "NL5.3", "NL5.4", "NL5.5", "NL6", "NL6.1", "NL6.2", "NL6.3", "NL6.4", "NL6.5","S1", "S1.1", "S1.2", "S1.3", "S1.4", "S1.5", "S2", "S2.1", "S2.2", "S2.3", "S2.4", "S2.5", "S3", "S3.1", "S3.2", "S3.3", "S3.4", "S3.5")

identical(rownames(nochim_seqtab.all), rownames(all_metadata)) ##this should be TRUE

rownames(nochim_seqtab.all)

rownames(all_metadata)


```
The rownames of the seqtab should be identical to the rownames of the metadata, you may or may not need to change the rownames like I did.



### Step 10: Variance stablilizing transformation

```{r make-count-tables, eval = FALSE}


# 
# #Give our seq headers more manageable names (ASV_1, ASV_2...) and to write some tables. 
# asv_seqs <- colnames(nochim_seqtab.all)
# asv_headers <- vector(dim(nochim_seqtab.all)[2], mode="character")
# 
# for (i in 1:dim(nochim_seqtab.all)[2]) {
#   asv_headers[i] <- paste(">ASV", i, sep="_")
# }
# 
# # making and writing out a fasta of our final ASV seqs:
# asv_fasta <- c(rbind(asv_headers, asv_seqs))
# write(asv_fasta, "ASVs.fa")
# asv_tab <- t(nochim_seqtab.all)  #transpose seq tab
# row.names(asv_tab) <- sub(">", "", asv_headers)
# write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)
# # count_tab will hold the ASV and sample names
# count_tab <- read.table("ASVs_counts.tsv", header=T, row.names=1,
#                         check.names=F, sep="\t")
# 
# 
# 
# # creating table of taxonomy [and setting any that are unclassified as "NA"...]
# write.table(taxid, "ASVs_taxonomy.tsv", sep = "\t", quote=F, col.names=NA)
# # tax_tab will hold the ASVs and the taxa that go with them.
# tax_tab <- as.matrix(read.table("ASVs_taxonomy.tsv", header=T,
#                                 row.names=1, check.names=F, sep="\t"))
# 
# 
# #check to see if these look reasonable. Note, if you have a lot of data, use 'head'
# head(count_tab)
# head(tax_tab)
# #At this stage, 'count_tab' has rows that are 'ASV_1' (for example), and columns that are 'name.fastq' and the data are counts.
# #At this stage, 'tax_tab' has rows that are 'ASV_1' (for example), and columns that are taxonomic classes and the data are the taxonomic lineages of each ASV. 
# 
# # If your count_tab does NOT have the sample names in the rows, then you need to fix that. 
# #The ASV data tables and the #metadata tables MUST be formatted so that the sample names are in rows (or more, they must both be oriented the same way #and the statistical people prefer samples in rows).
# #I guess the tax_tab should have the ASVs in the same orientation as the #count_tab. If you need to fix them...this script will do so. BUT, you should rename the original files so that you preserve them. I've done that by adding a 't' here, which is the command for transposing a table (so you'll know by the name that it's a transposed table).
# 
# count_tab_t <- t(count_tab)
# tax_tab_t <- t(tax_tab)
# 
# #They should now both have the sample name.fastq on the rows in these version of the tables.
# #It would be smart here to check the row names of your count_tab to make sure they're in the same order as your prepared metadata table. Cuz now's a good time to fix it if it's not!
# 
# identical(rownames(count_tab_t), rownames(all_metadata)) #if this = TRUE, then you're good

```

```{r variance-stabilizing-transfromation, eval=FALSE, message = F}
# 
# design <- as.formula(~spike)
# modelMartrix = model.matrix(design, data = all_metadata)
# all_metadata$sample = factor(all_metadata$sample, 
#                              levels = c("Antibiotics", "Fe(OH)3", "Control", "Na2SO4"))
# modelMatrix = model.matrix(design, data= all_metadata)
# 
# 
# deseq_counts <- DESeqDataSetFromMatrix(count_tab, colData = all_metadata, design = ~spike)
# deseq_counts <- estimateSizeFactors(deseq_counts, type = "poscounts")
# deseq_counts_vst <- varianceStabilizingTransformation(deseq_counts)
# vst_trans_count_tab <- assay(deseq_counts_vst) # so this is a transformed table 
# 



```


STEP 10- rewritten; 
The whole changing the count table to have easier ASV names is totally unnecessary.. 
Could skip the first two chunks and do just this one instead
```{r try-another-way-to-do-transformation, eval = FALSE}

nochim_seqtab.all <- readRDS("~/landfill-microcosms/data/live_samples_objects/nochim_seqtab.all")
taxid <- readRDS("~/landfill-microcosms/data/live_samples_objects/taxid")


design <- as.formula(~spike)

modelMartrix_killed <- model.matrix(design, data = all_metadata)

all_metadata$sample = factor(all_metadata$sample, 
                             levels = c("Antibiotics", "Fe(OH)3", "Control", "Na2SO4"))

modelMatrix= model.matrix(design, data= all_metadata)

nochim_seqtab.t <- t(nochim_seqtab.all)

deseq_counts <- DESeqDataSetFromMatrix(nochim_seqtab.t, colData = all_metadata, design = ~spike) 

deseq_counts <- estimateSizeFactors(deseq_counts, type = "poscounts")

deseq_counts_vst <- varianceStabilizingTransformation(deseq_counts)

vst_trans_count_tab <- assay(deseq_counts_vst) # so this is a transformed table 

saveRDS(vst_trans_count_tab, "~/landfill-microcosms/data/live_samples_objects/vst_trans_count_tab")

```

### Step 11: Make a phyloseq object W/ Variance Transformed seq tab from step 10 

The phyloseq object has 4 elements:

1) OTU (ASV) table aka a count table --> made in step 6 (untransformed) and step 10 (variance stablizing transformation)
2) taxatable: tax_table -->made in step 7.1
3) metadata (loaded in step 9)
4) A phylogenetic tree (step 8- optional)

```{r prepare_objects, eval = FALSE}

vst_trans_count_tab.t = t(vst_trans_count_tab) #transpose the transformed counts tab

identical(rownames(vst_trans_count_tab.t), rownames(all_metadata)) #should be true


```

```{r make-variance-stablizing-trans-phyloseq-object, eval = FALSE}

nochim_seqtab.all <- readRDS("~/landfill-microcosms/data/live_samples_objects/nochim_seqtab.all")
taxid <- readRDS("~/landfill-microcosms/data/live_samples_objects/taxid")
all_fitGTR <- readRDS("~/landfill-microcosms/data/live_samples_objects/all_fitGTR")
vst_trans_count_tab <-readRDS("~/landfill-microcosms/data/live_samples_objects/vst_trans_count_tab")

vst_all <- phyloseq(otu_table(vst_trans_count_tab, taxa_are_rows = T),
                    sample_data(all_metadata),
                    tax_table(taxid),
                    phy_tree(all_fitGTR$tree))

```

#Step 11.1: Make a phyloseq object w/o Variance Transformed data 
(Continued from step 7, 8, 9 (no step 10)) might have to re-do some of these steps (line 744 subsetting the phyloseq object)
```{r make-untransformed-physeq-object, eval = FALSE}

nochim_seqtab.all <- readRDS("~/landfill-microcosms/data/live_samples_objects/nochim_seqtab.all")
taxid <- readRDS("~/landfill-microcosms/data/live_samples_objects/taxid")


ps_all = phyloseq(otu_table(nochim_seqtab.all, taxa_are_rows = F), sample_data(all_metadata), tax_table(taxid), phy_tree(all_fitGTR$tree))

```


#save phyloseq objects onto the desktop
```{r, eval = FALSE}

saveRDS(vst_all, file = "../data/live_samples_objects/vst_all")
saveRDS(ps_all, file = "../data/live_samples_objects/ps_all")

```




